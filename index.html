<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MonoAvatar++ learns a real-time high-quality implicit 3D head avatar from a monocular RGB video captured in the wild.">
  <meta name="keywords" content="MonoAvatar++">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="MonoAvatar++: Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes" />
  <meta property="og:description" content="MonoAvatar++ learns a real-time high-quality implicit 3D head avatar from a monocular RGB video captured in the wild." />
  <meta property="og:image" content="./static/images/teaser.png" />
  <title>MonoAvatar++: Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-T834G2HR2M"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-T834G2HR2M');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MonoAvatar++: Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</h1>
          <h1 class="title is-size-3" style="color:#5a6268;">CVPR 2024</h1>
          <div class="is-size-5 publication-authors" style="text-align: center; font-size: 25px; padding-top: 10px;">
            <span class="author-block">
              <a href="https://zqbai-jeremy.github.io">Ziqian Bai</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=qsrpuKIAAAAJ">Feitong Tan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.seanfanello.it/">Sean Fanello</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=lJ3VfV8AAAAJ&hl=en">Rohit Pandey</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DB8aKRgAAAAJ&hl=en">Mingsong Dou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://shichenliu.github.io/">Shichen Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=XhyKVFMAAAAJ&hl=en">Ping Tan</a><sup>3</sup>,
            </span>            
            <span class="author-block">
              <a href="https://www.zhangyinda.com/">Yinda Zhang</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="text-align: center;">
            <span class="author-block"><sup>1</sup> Google,</span>
            <span class="author-block"><sup>2</sup> Simon Fraser University,</span>
            <span class="author-block"><sup>3</sup> The Hong Kong University of Science and Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./9485.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="./9485_supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.01436"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://zqbai-jeremy.github.io/autoavatar/static/images/video_arxiv.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/facebookresearch/AutoAvatar"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>
      
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container" style="margin-left: -170px; margin-right: -170px;">
        <img id="teaser" class="img-responsive img-rounded" src="./static/images/teaser.png" style="width:200%" alt="">
        <h2 class="subtitle has-text-justified">
          <b>MonoAvatar++</b> builds a 3D avatar representation of a person that can be rendered in real-time, from just a single short monocular RGB video (e.g., 1-2 minutes).
          We leverage a 3DMM to track the user's expressions, and construct an efficient volumetric 3D avatar that can be rendered in >= 30 FPS with 512 x 512 
          resolutions, given user-defined expression and viewpoint.
          <!-- Note that our method works well on challenging materials, e.g., hair, and dramatic expressions. -->
        </h2>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="columns is-centered has-text-centered" style="padding-top: 100px;">
      <div class="column">
        <div class="column content">
          <video class="video_vertical_aign"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject15.mp4"
                    type="video/mp4">
          </video>
          <div class="container" style="margin-top: -90px; font-size: 20px;">
            Input Driving Video &emsp;&emsp;&emsp;Rendered Avatar &emsp;&emsp;&emsp;Rendered Depth
          </div>
        </div>
      </div>

      <div class="column">
        <div class="column content">
          <video class="video_vertical_aign"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject18.mp4"
                    type="video/mp4">
          </video>
          <div class="container" style="margin-top: -168px; font-size: 20px;">
            Input Driving Video &emsp;&emsp;&emsp;Rendered Avatar &emsp;&emsp;&emsp;Rendered Depth
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. 
            However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly 
            in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural 
            rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, 
            such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit 
            head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. 
            Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an 
            underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting 
            in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight 
            MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach 
            runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div id="overview" class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">Overview</h2></div>
      <div class="container" style="margin-left: -170px; margin-right: -170px;">
        <img class="img-responsive img-rounded" src="./static/images/overview.png" style="width:100%" alt="">
        <h2 class="content has-text-justified">
          Overview of our pipeline. Our core avatar representation is Mesh-anchored Hash Table Blendshapes (Sec.3.1), where multiple 
          small hash tables are attached to each 3DMM vertex. During inference, our method starts from a displacement map encoding 
          the facial expression, which is then fed into a U-Net to predict hash table weights and per-vertex features. The predicted 
          weights are used to linearly combine the multiple hash tables attached on each 3DMM vertex (Sec.3.2). During volumetric 
          rendering (Sec.3.3), for each query point, we search its k-nearest-neighbor vertices, then pull embeddings from the merged 
          hash tables and concatenate with the per-vertex feature to decode local density and color via a tiny MLP with two hidden layers.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -100px;">
  <div class="container is-four-fifths">
    <!--/ Results. -->
    <div id="results" class="hero-body">
      <!-- <div class="columns is-centered has-text-centered"><h2 class="title is-3">Results</h2></div> -->

      <div class="container" style="text-align: center;">
        <hr>
        <br>
        <h3 class="title is-3 has-text-centered">Results of comparing with the state-of-the-art approaches.</h3>
        <p style="font-size: 20px; text-align:center">Compare with PointAvatar[<a href='#ref1'>1</a>], INSTA[<a href='#ref2'>2</a>], 
          NeRFBlendshape[<a href='#ref3'>3</a>], MonoAvatar[<a href='#ref4'>4</a>].</p>
        <br>

        Our method is able to achieve one of the best rendering quality while maintaining real-time rendering speed.

        <div class="opacity-off">
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject15_vs_sota.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="opacity-off">
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject18_vs_sota.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="opacity-off">
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject2_vs_sota.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="opacity-off">
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject14_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject19_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject8_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject0_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject16_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject4_vs_sota.mp4"
                    type="video/mp4">
          </video>

          <br>
          <video width="1500"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject11_vs_sota.mp4"
                    type="video/mp4">
          </video>

        </div>
        <br>

        <hr>
        <br>
        <h3 class="title is-3 has-text-centered">More Results of Our Approach.</h3>
        <p style="font-size: 20px; text-align:center">
          Labels - <b>Left:</b> Input Driving Video,  <b>Center:</b> Rendered Avatar,  <b>Right:</b> Rendered Depth
        </p>
        <br>

        <div class="opacity-off">

          <video width="740"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject15.mp4"
                    type="video/mp4">
          </video>
          <br>
          <video width="740"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject18.mp4"
                    type="video/mp4">
          </video>
          <br>
          <video width="740"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject8.mp4"
                    type="video/mp4">
          </video>
          <br>
          <video width="740"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject4.mp4"
                    type="video/mp4">
          </video>
          <br>
          <video width="740"
                height="310"
                controls
                autoplay
                muted
                loop>
            <source src="videos/subject11.mp4"
                    type="video/mp4">
          </video>
          <br>
        </div>

        <br>


        <hr>
        <br>

        <h3 class="title is-3 has-text-centered">Real-time Live Demo of Avatar Driving (Stereo Rendering).</h3>
        <p style="font-size: 20px; text-align:center">
          Labels - <b>Left:</b> Driving Video,  <b>Center:</b> Rendered Avatar,  <b>Right:</b> Viewing in Headset
        </p>
        <br>

        We build a real-time demo based on our method, where we track the facial performance of the actor with a webcam, and 
        render our avatar on a workstation with a RTX3080Ti. Finally, we display the rendered stereo pair on a headset and a 
        webpage. The tracking results and rendered videos are streaming through the internet, which may cause latency between 
        the driving and the rendering, and slightly unsmoothed videos.

        <div class="opacity-off">

          <video width="1500"
                height="319"
                controls
                autoplay
                muted
                loop>
            <source src="videos/demo.mp4"
                    type="video/mp4">
          </video>
        </div>

        <br>
        <hr>
        <br>

        <h3 id="references" class="title is-3 has-text-centered">References</h3>

        <div style="text-align:left">
          <span id="ref1">[1] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J Black, and Otmar Hilliges.
          Pointavatar: Deformable point based head avatars from videos. In CVPR, 2023. <a href="https://zhengyuf.github.io/PointAvatar/" target="_blank">[link]</a></span>
    
          <br>
    
          <span id="ref2">[2] Wojciech Zielonka, Timo Bolkart, and Justus Thies.
          Instant volumetric head avatars. In CVPR, 2023. <a href="https://zielon.github.io/insta/" target="_blank">[link]</a></span>
          
          <br>
    
          <span id="ref3">[3] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang.
          Reconstructing personalized semantic facial nerf models from monocular video. In SIGGRAPH Asia, 2022. <a href="https://ustc3dv.github.io/NeRFBlendShape/" target="_blank">[link]</a></span>
    
          <br>
    
          <span id="ref4">[4] Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka,
          Ruofei Du, Ming song Dou, Sergio Orts-Escolano, et al.
          Learning personalized high quality volumetric head avatars from monocular rgb videos. In CVPR, 2023. <a href="https://augmentedperception.github.io/monoavatar/" target="_blank">[link]</a></span>

        </div>

        <br>
        <hr>
        <br>

      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2203.13817.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            This website borrows the template from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
